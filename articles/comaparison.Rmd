---
title: "Social media R api-wrappers comparision"
author: "Ryszard Szymański, Zuzanna Magierska, Andrzej Nowikowski, Rafał Muszyński, Piotr Janus"
date: "5 01 2020"
output: html_document
---

```{r setup, include=FALSE, cache=TRUE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(grid)
library(png)
file_rd <- tempfile(fileext = ".png")
file_hn <- tempfile(fileext = ".png")
file_tw <- tempfile(fileext = ".png")
ff <- tempfile(fileext = ".jpg")


## Download necessary images for plots
download.file("https://www.iconfinder.com/icons/2062086/download/png/256", file_rd , mode = 'wb')
download.file("https://www.iconfinder.com/icons/2613275/download/png/256", file_hn , mode = 'wb')
download.file("https://www.freeiconspng.com/download/47450", file_tw , mode = 'wb')
download.file("https://i.stack.imgur.com/M2jeo.jpg", ff , mode = 'wb')

tw_img <- readPNG(file_tw, native = TRUE)
rd_img <- readPNG(file_rd, native = TRUE)
hn_img <- readPNG(file_hn, native = TRUE)
```


```{r message=FALSE, warning=FALSE, echo=FALSE}

library(tidytext)
library(tidyr)
library(dplyr)
library(xml2)
library(rvest)
library(stringr)
library(plotly)
library(ggplot2)

## Funcs for cleaning comments
clean_sentences <- function(sentences) {
  sentences %>%
    remove_html() %>%
    str_to_lower() %>%
    str_replace_all('[^A-Z|a-z]', ' ') %>%
    str_replace_all('\\s\\s*', ' ') %>%
    str_split(' ', simplify = TRUE)
}


clear_all_sentences <- function(sentences) {

  clear_single <- function(sentence) {
    words <- sentence %>%
      clean_sentences() %>%
      remove_stop_words()

  }

  sapply(sentences, clear_single, USE.NAMES = FALSE)
}

remove_html <- function(texts) {
  html_strings <- sprintf("<body>%s<body>", texts)
  sapply(html_strings, function(html_string) {
    html_string %>%
      read_html() %>%
      html_text()
  }, USE.NAMES = FALSE)
}

remove_stop_words <- function(words) {
  stop_words <- tidytext::stop_words
  words[!words %in% c("", " ", stop_words$word)]
}

# Get sentiment using sentimentr
get_sentiment2 <- function(sentence) {
  sent <- sentimentr::sentiment(sentence)
  res <- mean(sent$sentiment)
  unname(res)
}

get_sentiment_vec <- function(sentences) {
  sapply(sentences, get_sentiment2)
}

# Function copied form RedditExtractoR package, modified to return dates as POSIXct
# Detail date is required for user actions analysis
# Originaly date was retured as %Y-%m-%d
reddit_content2 <- function (URL, wait_time = 2)
{
  if (is.null(URL) | length(URL) == 0 | !is.character(URL)) {
    stop("invalid URL parameter")
  }
  GetAttribute = function(node, feature) {
    Attribute = node$data[[feature]]
    replies = node$data$replies
    reply.nodes = if (is.list(replies))
      replies$data$children
    else NULL
    return(list(Attribute, lapply(reply.nodes, function(x) {
      GetAttribute(x, feature)
    })))
  }
  get.structure = function(node, depth = 0) {
    if (is.null(node)) {
      return(list())
    }
    filter = is.null(node$data$author)
    replies = node$data$replies
    reply.nodes = if (is.list(replies))
      replies$data$children
    else NULL
    return(list(paste0(filter, " ", depth), lapply(1:length(reply.nodes),
                                                   function(x) get.structure(reply.nodes[[x]], paste0(depth,
                                                                                                      "_", x)))))
  }
  data_extract = data.frame(id = numeric(), structure = character(),
                            post_date = as.Date(character()), comm_date = as.Date(character()),
                            num_comments = numeric(), subreddit = character(), upvote_prop = numeric(),
                            post_score = numeric(), author = character(), user = character(),
                            comment_score = numeric(), controversiality = numeric(),
                            comment = character(), title = character(), post_text = character(),
                            link = character(), domain = character(), URL = character())
  pb = utils::txtProgressBar(min = 0, max = length(URL), style = 3)
  for (i in seq(URL)) {
    if (!grepl("^https?://(.*)", URL[i]))
      URL[i] = paste0("https://www.", gsub("^.*(reddit\\..*$)",
                                           "\\1", URL[i]))
    if (!grepl("\\?ref=search_posts$", URL[i]))
      URL[i] = paste0(gsub("/$", "", URL[i]), "/?ref=search_posts")
    X = paste0(gsub("\\?ref=search_posts$", "", URL[i]),
               ".json?limit=500")
    raw_data = tryCatch(RJSONIO::fromJSON(readLines(X, warn = FALSE)),
                        error = function(e) NULL)
    if (is.null(raw_data)) {
      Sys.sleep(min(1, wait_time))
      raw_data = tryCatch(RJSONIO::fromJSON(readLines(X,
                                                      warn = FALSE)), error = function(e) NULL)
    }
    if (is.null(raw_data) == FALSE) {
      meta.node = raw_data[[1]]$data$children[[1]]$data
      main.node = raw_data[[2]]$data$children
      # browser()
      if (min(length(meta.node), length(main.node)) >
          0) {
        structure = unlist(lapply(1:length(main.node),
                                  function(x) get.structure(main.node[[x]],
                                                            x)))
        TEMP = data.frame(id = NA, structure = gsub("FALSE ",
                                                    "", structure[!grepl("TRUE", structure)]),
                          post_date = as.POSIXct(meta.node$created_utc,
                                                 origin = "1970-01-01"), comm_date = as.POSIXct(unlist(lapply(main.node,
                                                                                                              function(x) {
                                                                                                                GetAttribute(x, "created_utc")
                                                                                                              })), origin = "1970-01-01"),
                          num_comments = meta.node$num_comments, subreddit = ifelse(is.null(meta.node$subreddit),
                                                                                    "UNKNOWN", meta.node$subreddit), upvote_prop = meta.node$upvote_ratio,
                          post_score = meta.node$score, author = meta.node$author,
                          user = unlist(lapply(main.node, function(x) {
                            GetAttribute(x, "author")
                          })), comment_score = unlist(lapply(main.node,
                                                             function(x) {
                                                               GetAttribute(x, "score")
                                                             })), controversiality = unlist(lapply(main.node,
                                                                                                   function(x) {
                                                                                                     GetAttribute(x, "controversiality")
                                                                                                   })), comment = unlist(lapply(main.node,
                                                                                                                                function(x) {
                                                                                                                                  GetAttribute(x, "body")
                                                                                                                                })), title = meta.node$title, post_text = meta.node$selftext,
                          link = meta.node$url, domain = meta.node$domain,
                          URL = URL[i], stringsAsFactors = FALSE)
        TEMP$id = 1:nrow(TEMP)
        if (dim(TEMP)[1] > 0 & dim(TEMP)[2] > 0)
          data_extract = rbind(TEMP, data_extract)
        else print(paste("missed", i, ":", URL[i]))
      }
    }
    utils::setTxtProgressBar(pb, i)
    Sys.sleep(min(2, wait_time))
  }
  close(pb)
  return(data_extract)
}


```

We will be comparing comments about "AMD at CES 2020" on hackernews, reddit and twitter. 

Hackernews story:
https://news.ycombinator.com/item?id=21990964

Reddit story:
https://www.reddit.com/r/Amd/comments/el0uyn/amd_ces_2020_press_conference_megathread/

Twitter story:
* starting from this post: https://twitter.com/LisaSu/status/1214397223137501185
and searching #AMD#CES2020


```{r, echo=FALSE, message=FALSE, warning=FALSE}
## put url and ids
hacker_id <- 21990964
reddit_url <- "https://www.reddit.com/r/Amd/comments/el0uyn/amd_ces_2020_press_conference_megathread/"
# reddit_url <- "https://www.reddit.com/r/Amd/comments/elxiac/amds_third_shoe_finally_drops_at_ces_20207nm_zen/"
twitter_id <- 1214397223137501185
# twitter_id <- 1216495119702073347 or this one 
twitter_username <- "AMD"
twitter_tags <- "#AMD#CES2020"
```

```{r, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}
# download hacker news data
library(hackeRnews)
hacker_news <- get_item_by_id(hacker_id)
hacker_comments <- get_comments(hacker_news)
```

```{r, include=FALSE, cache=TRUE}
# download reddit data
library(RedditExtractoR)
reddit_data <- reddit_content2(reddit_url)
# this caused problems on some machines
reddit_data <- reddit_data %>% filter(validUTF8(comment))
```

```{r, include=FALSE, cache=TRUE}
# download twitter data
library(twitteR)
source("twitter_access_tokens.R")

setup_twitter_oauth(consumerKey, consumerSecret, access_token, access_secret)

## fetching all the reply to user
tw <- twitteR::searchTwitter(twitter_tags, sinceID = twitter_id, n = 74)
twitter_data <- twListToDF(tw)

# remove words starting with @
twitter_data$text <- gsub("@\\w+ *", "", twitter_data$text)
```


```{r message=FALSE, warning=FALSE, echo=FALSE}
# Get sentiment
twitter_data <- twitter_data %>%   mutate(sentiment = get_sentiment_vec(text) )
# maybe get tweets with highest and lowest sentiment

reddit_data <- reddit_data %>% mutate(sentiment = get_sentiment_vec(comment) )

hacker_comments <- hacker_comments %>% mutate(sentiment = get_sentiment_vec(text))
```

Comapare activity of users on selected platforms. TwitteR allows fetching only 74 comments.
```{r, message=FALSE, warning=FALSE, echo=FALSE}
library(grid)

# Twitter responses
tw_comments <- nrow(twitter_data)
names(tw_comments) <- "Twitter"

# Reddit responses
reddit_comments <- nrow(reddit_data)
names(reddit_comments) <- "Reddit"

# HackerNews responses
hn_comments <- nrow(hacker_comments)
names(hn_comments) <- "HackerNews"

all_comments <- data.frame(comments = c(hn_comments, reddit_comments, tw_comments), source = names(c(hn_comments, reddit_comments, tw_comments)))

ggplot(all_comments, aes(x = source, y = comments)) +
  geom_col(fill = c("darkorange", "red", "skyblue"), color="black", size=1) +
  xlab("") +
  theme_minimal() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotation_custom(rasterGrob(hn_img), xmin=-1.6, ymin = all_comments$comments[1]-50, ymax = all_comments$comments[1]+50) +
  annotation_custom(rasterGrob(rd_img), xmin= 0.4, ymin = all_comments$comments[2]-50, ymax = all_comments$comments[2]+50)+
  annotation_custom(rasterGrob(tw_img), xmin= 2.4, ymin = all_comments$comments[3]-50, ymax = all_comments$comments[3]+50)
```

Sentiment analysis of comments on all platforms
```{r, message=FALSE, warning=FALSE, echo=FALSE}

library(ggridges)
hacker_sentiment <- data.frame(source = "HackerNews", sentiment = hacker_comments$sentiment)
reddit_sentiment <- data.frame(source = "Reddit", sentiment = reddit_data$sentiment)
twitter_sentiment <- data.frame(source = "Twitter", sentiment = twitter_data$sentiment)
all_sentiments <- rbind(hacker_sentiment, reddit_sentiment, twitter_sentiment)

ggplot(all_sentiments, aes(x = sentiment, y = source)) +
  geom_density_ridges(aes(fill = source), alpha = 0.6, from=-1) +
  scale_fill_manual(values = c("darkorange", "red", "skyblue")) +
  theme_minimal() +
  scale_y_discrete(expand = c(0.01,0)) +
  theme(legend.position = "none")
```

Count unique users who commented on news
```{r, warning=FALSE, echo=FALSE, message=FALSE}
# if waffle rises error

library(waffle)
library(extrafont)
fa_font <- tempfile(fileext = ".ttf")

download.file("http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/fonts/fontawesome-webfont.ttf?v=4.3.0",
              destfile = fa_font, method = "curl")

font_import(paths = dirname(fa_font), prompt = FALSE)

# Change if you are not win user
# loadfonts(device = "win")

# Twitter users
tw_usr <- length(unique(twitter_data$created))
names(tw_usr) <- "Twitter"

# Reddit users
rd_usr <- length(unique(reddit_data$user))
names(rd_usr) <- "Reddit"

# HackerNews users
hn_usr <- length(unique(hacker_comments$by))
names(hn_usr) <- "HackerNews"

all_users <- c(hn_usr, rd_usr, tw_usr)

# need to make adjustments with rows na divide number
waffle(all_users/11.01, rows=5, use_glyph = "child", glyph_size = 5, colors = c("darkorange", "red", "skyblue"))
```

Let's compare, how the evolved on each platform
```{r, message=FALSE, echo=FALSE, warning=FALSE}
library(zoo)

# standarize time column name
reddit_data <- reddit_data %>%  mutate(time = comm_date)
twitter_data <- twitter_data %>% mutate(time = created)

# calculate rolling sentiment (windowed average)
get_rolling <- function(df){
  df %>% 
    mutate(roll_sentiment = rollmean(sentiment, 10, na.pad = TRUE)) %>% 
    arrange(time) %>% 
    group_by(time) %>% 
    filter(!is.na(roll_sentiment))
}

hacker_comments_roll <- get_rolling(hacker_comments)
reddit_comments_roll <- get_rolling(reddit_data)
twitter_comments_roll <- get_rolling(twitter_data)

get_loess <- function(df){
  start_date <- min(df$time)
  as.data.frame(loess.smooth(df$time, df$roll_sentiment, span = 1/15, degree = 1,family =
                                         c("symmetric", "gaussian"), evaluation = 50)) %>% 
  mutate(x = difftime(as.POSIXct(x, origin="1970-01-01"), start_date, units = "mins"))
}

loess.hn <- get_loess(hacker_comments_roll)
loess.rd <- get_loess(reddit_comments_roll)
loess.tw <- get_loess(twitter_comments_roll)

g <- ggplot() +
  geom_line(data = loess.hn, aes(x = x, y = y), size=1, col="orange") +
  geom_line(data = loess.rd, aes(x = x, y = y), size=1, col="red") +
  geom_line(data = loess.tw, aes(x = x, y = y), size=1, col="skyblue") +
  ylab("activity") + 
  xlab("minutes passed") +
  theme_minimal()

ggplotly(g)
```

Overall discussion 
```{r, warning=FALSE, echo=FALSE, message=FALSE}
library(lubridate)

get_cumulative_activity_in_time <- function(df, name){
  df <- df %>% arrange(time)
  df$cumulative_activity <- as.numeric(row.names(df))
  df <- df %>% mutate(time = ymd_hms(time))
  df <- cbind(df, data.frame(site = name))
  df %>% select(site, cumulative_activity, time)
}

cumulative_activity_in_time <- rbind(
  get_cumulative_activity_in_time(twitter_data, "Twitter"),
  get_cumulative_activity_in_time(reddit_data, "Reddit"),
  get_cumulative_activity_in_time(hacker_comments, "Hacker News")
) 


g <- cumulative_activity_in_time %>% 
  ggplot(aes(x = time, y = cumulative_activity, color = site, group=site)) +
  geom_line() +
  geom_point(size=2) +
  theme_minimal() +
  xlab("") +
  ylab("Cumulative activity") +
  theme(panel.grid.major.x  = element_blank(), panel.grid.minor = element_blank())
  
  

ggplotly(g) %>% config(displayModeBar = F)


```

# Wordclouds
```{r ,warning=FALSE, echo=FALSE, message=FALSE}
library(ggwordcloud)

get_word_cloud <- function(data){
  data <- table(unlist(clear_all_sentences(data)))
  data <- data.frame(data) %>% top_n(300, Freq)
  data %>% 
    mutate(word=Var1) %>% 
    mutate(n=Freq) %>% 
    mutate(color=factor(sample(10,nrow(data), replace=TRUE))) %>% 
    ggplot(aes(label=word, size=n, color=color)) + 
      geom_text_wordcloud()
}
```

Hackernews
```{r}
get_word_cloud(hacker_comments$text)
```
Reddit
```{r}
get_word_cloud(reddit_data$comment)
```
Twitter
```{r}
get_word_cloud(twitter_data$text)
```
