---
title: "Social media R api-wrappers comparision"
author: "Ryszard Szymański, Zuzanna Magierska, Andrzej Nowikowski, Rafał Muszyński, Piotr Janus"
date: "5 01 2020"
output: 
  html_document:
    theme: united
    toc: true
    highlight: tango
    self-contained: true
---

```{r setup, include=FALSE, cache=TRUE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(grid)
library(png)
file_rd <- tempfile(fileext = ".png")
file_hn <- tempfile(fileext = ".png")
file_tw <- tempfile(fileext = ".png")
ff <- tempfile(fileext = ".jpg")


## Download necessary images for plots
download.file("https://www.iconfinder.com/icons/2062086/download/png/256", file_rd , mode = 'wb')
download.file("https://www.iconfinder.com/icons/2613275/download/png/256", file_hn , mode = 'wb')
download.file("https://www.freeiconspng.com/download/47450", file_tw , mode = 'wb')
download.file("https://i.stack.imgur.com/M2jeo.jpg", ff , mode = 'wb')

tw_img <- readPNG(file_tw, native = TRUE)
rd_img <- readPNG(file_rd, native = TRUE)
hn_img <- readPNG(file_hn, native = TRUE)
```


```{r message=FALSE, warning=FALSE, echo=FALSE}

library(tidytext)
library(tidyr)
library(dplyr)
library(xml2)
library(rvest)
library(stringr)
library(plotly)
library(ggplot2)

## Funcs for cleaning comments
clean_sentences <- function(sentences) {
  sentences %>%
    remove_html() %>%
    str_to_lower() %>%
    str_replace_all('[^A-Z|a-z]', ' ') %>%
    str_replace_all('\\s\\s*', ' ') %>%
    str_split(' ', simplify = TRUE)
}


clear_all_sentences <- function(sentences) {

  clear_single <- function(sentence) {
    words <- sentence %>%
      clean_sentences() %>%
      remove_stop_words()

  }

  sapply(sentences, clear_single, USE.NAMES = FALSE)
}

remove_html <- function(texts) {
  html_strings <- sprintf("<body>%s<body>", texts)
  sapply(html_strings, function(html_string) {
    html_string %>%
      read_html() %>%
      html_text()
  }, USE.NAMES = FALSE)
}

remove_stop_words <- function(words) {
  stop_words <- tidytext::stop_words
  words[!words %in% c("", " ", stop_words$word)]
}

# Get sentiment using sentimentr
get_sentiment2 <- function(sentence) {
  sent <- sentimentr::sentiment(sentence)
  res <- mean(sent$sentiment)
  unname(res)
}

get_sentiment_vec <- function(sentences) {
  sapply(sentences, get_sentiment2)
}

```

We will be comparing comments about "AMD at CES 2020" on hackernews, reddit and twitter. 

Hackernews story:
https://news.ycombinator.com/item?id=21990964

Reddit story:
https://www.reddit.com/r/Amd/comments/el0uyn/amd_ces_2020_press_conference_megathread/

Twitter story:
* starting from this post: https://twitter.com/LisaSu/status/1214397223137501185
and searching #AMD#CES2020


```{r, echo=FALSE, message=FALSE, warning=FALSE}
## put url and ids
hacker_id <- 21990964
reddit_url <- "https://www.reddit.com/r/Amd/comments/el0uyn/amd_ces_2020_press_conference_megathread/"
# reddit_url <- "https://www.reddit.com/r/Amd/comments/elxiac/amds_third_shoe_finally_drops_at_ces_20207nm_zen/"
twitter_id <- 1214397223137501185
# twitter_id <- 1216495119702073347 or this one 
twitter_username <- "AMD"
twitter_tags <- "#AMD #CES2020"
```

```{r, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}
# download hacker news data
library(hackeRnews)
future::plan(future::multiprocess)
hacker_news <- get_item_by_id(hacker_id)
hacker_comments <- get_comments(hacker_news)
```

```{r, include=FALSE, cache=TRUE}
# download reddit data
library(RedditExtractoR)
reddit_data <- reddit_content2(reddit_url)
# this caused problems on some machines
reddit_data <- reddit_data %>% filter(validUTF8(comment))
```

```{r, include=FALSE, cache=TRUE}
# download twitter data
library(rtweet)
source("twitter_access_tokens.R")

twitteR::setup_twitter_oauth(consumerKey, consumerSecret, access_token, access_secret)

## fetching all the reply to user
toDate <- format(Sys.time() - 60 * 60 * 24 * 7, "%Y%m%d%H%M")

twitter_data <- search_30day(twitter_tags, env_name = "news", n = 3000, toDate = toDate)
a <- twitter_data
twitter_data <- a
# remove words starting with @ and filter for english comments
twitter_data <- twitter_data %>% 
  mutate(text = gsub("@\\w+ *", "", text)) %>% 
  filter(validUTF8(text), lang == "en", !duplicated(text)) %>% 
  select(screen_name, created_at, text) 

# filter for this year tweets
twitter_data <- twitter_data %>% filter(created_at > "2020-01-04")
```


```{r message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}
# Get sentiment
twitter_data <- twitter_data %>%   mutate(sentiment = get_sentiment_vec(text))
# maybe get tweets with highest and lowest sentiment

reddit_data <- reddit_data %>% mutate(sentiment = get_sentiment_vec(comment))

hacker_comments <- hacker_comments %>% mutate(sentiment = get_sentiment_vec(text))
```

Comapare activity of users on selected platforms. TwitteR allows to fetch only 74 comments.

```{r, message=FALSE, warning=FALSE, echo=FALSE, fig.width=10, fig.align="center"}
library(grid)

# Twitter responses
tw_comments <- nrow(twitter_data %>% select(text) %>% unique())
names(tw_comments) <- "Twitter"

# Reddit responses
reddit_comments <- nrow(reddit_data %>%  select(comment) %>% unique())
names(reddit_comments) <- "Reddit"

# HackerNews responses
hn_comments <- nrow(hacker_comments %>% select(text) %>% unique())
names(hn_comments) <- "HackerNews"

all_comments <- data.frame(comments = c(hn_comments, reddit_comments, tw_comments), source = names(c(hn_comments, reddit_comments, tw_comments)))

ggplot(all_comments, aes(x = source, y = comments)) +
  geom_col(fill = c("darkorange", "red", "skyblue"), color="black", size=1) +
  xlab("") +
  ylim(0, 550) +
  theme_minimal() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  annotation_custom(rasterGrob(hn_img), xmin=-1.6, ymin = all_comments$comments[1]-50, ymax = all_comments$comments[1]+50) +
  annotation_custom(rasterGrob(rd_img), xmin= 0.4, ymin = all_comments$comments[2]-50, ymax = all_comments$comments[2]+50)+
  annotation_custom(rasterGrob(tw_img), xmin= 2.4, ymin = all_comments$comments[3]-50, ymax = all_comments$comments[3]+50)
```

Sentiment analysis of comments on all platforms

```{r, message=FALSE, warning=FALSE, echo=FALSE, fig.width=10, fig.align="center"}

library(ggridges)
hacker_sentiment <- data.frame(source = "HackerNews", sentiment = hacker_comments$sentiment)
reddit_sentiment <- data.frame(source = "Reddit", sentiment = reddit_data$sentiment)
twitter_sentiment <- data.frame(source = "Twitter", sentiment = twitter_data$sentiment)
all_sentiments <- rbind(hacker_sentiment, reddit_sentiment, twitter_sentiment)

ggplot(all_sentiments, aes(x = sentiment, y = source)) +
  geom_density_ridges(aes(fill = source), alpha = 0.6, from=-1) +
  scale_fill_manual(values = c("darkorange", "red", "skyblue")) +
  theme_minimal() +
  scale_y_discrete(expand = c(0.01,0)) +
  theme(legend.position = "none")
```

Count unique users who commented on news

```{r, warning=FALSE, echo=FALSE, message=FALSE, fig.width=10, fig.align="center"}
# if waffle rises error

library(waffle)
library(extrafont)
fa_font <- tempfile(fileext = ".ttf")

download.file("http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/fonts/fontawesome-webfont.ttf?v=4.3.0",
              destfile = fa_font, method = "curl")

font_import(paths = dirname(fa_font), prompt = FALSE)

# Change if you are not win user
# loadfonts(device = "win")

# Twitter users
tw_usr <- length(unique(twitter_data$screen_name))
names(tw_usr) <- "Twitter"

# Reddit users
rd_usr <- length(unique(reddit_data$user))
names(rd_usr) <- "Reddit"

# HackerNews users
hn_usr <- length(unique(hacker_comments$by))
names(hn_usr) <- "HackerNews"

all_users <- c(hn_usr, tw_usr, rd_usr)

# need to make adjustments with rows na divide number
waffle(all_users/8.5, rows=4, use_glyph = "child", glyph_size = 7, colors = c("darkorange", "skyblue", "red"))
```



```{r, warning=FALSE, message=FALSE, echo=FALSE}
library(kableExtra)
hn_nchar <- sapply(hacker_comments$text, function(x) nchar(x)) %>% 
  na.omit() %>% 
  data.frame(nchar = .) %>% 
  mutate(source = "HackerNews")

rd_nchar <- sapply(reddit_data$comment, function(x) nchar(x)) %>% 
  na.omit() %>% 
  data.frame(nchar = .) %>% 
  mutate(source = "Reddit")

tw_nchar <- sapply(twitter_data$text, function(x) nchar(x)) %>% 
  na.omit() %>% 
  data.frame(nchar = .) %>% 
  mutate(source = "Twitter")

all_nchar <- rbind(hn_nchar, rd_nchar, tw_nchar) 

```

We can see that the majority of users who commented on "AMD at CES 2020" news did this on Reddit. However they were not too expansive, comapring to Hacker News users, whose avergage comment was the longest among others, reaching almost 300 characters. 


```{r message=FALSE, echo=FALSE, warning=FALSE, fig.width=10, fig.align="center"}

all_nchar %>% 
  group_by(source) %>% 
  summarise(`comment length` =  round(mean(nchar), 2)) %>% 
  kable(caption = "Average comment length") %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F)
```

But it was Twitter users who were the most consistent. That's beacuse of the fact that more than 50% of comments has length between 95 and 170 characters  

```{r message=FALSE, echo=FALSE, warning=FALSE, fig.width=10, fig.align="center"}

all_nchar %>% ggplot(aes(x=nchar, fill=source)) +
  geom_density(alpha=0.7) +
  theme_minimal() +
  xlab("Length of the comment") +
  scale_fill_manual(values = c("darkorange", "red", "skyblue"),
                     labels = c("HackerNews", "Reddit", "Twitter")) +
  scale_x_continuous(limits = c(0,500))

```

Let's compare, how it evolved on each platform

```{r, message=FALSE, echo=FALSE, warning=FALSE, fig.width=10, fig.align="center"}
library(zoo)

# standarize time column name
reddit_data <- reddit_data %>%  mutate(time = comm_date)
twitter_data <- twitter_data %>% mutate(time = created_at)

# calculate rolling sentiment (windowed average)
get_rolling <- function(df){
  df %>% 
    mutate(roll_sentiment = rollmean(sentiment, 10, na.pad = TRUE)) %>% 
    arrange(time) %>% 
    group_by(time) %>% 
    filter(!is.na(roll_sentiment)) %>% 
    select(time, roll_sentiment)
}

hacker_comments_roll <- get_rolling(hacker_comments)
reddit_comments_roll <- get_rolling(reddit_data)
twitter_comments_roll <- get_rolling(twitter_data)

get_loess <- function(df){
  start_date <- min(df$time)
  as.data.frame(loess.smooth(df$time, df$roll_sentiment, span = 1/10, degree = 3,family =
                                         c("symmetric", "gaussian"), evaluation = 50)) %>% 
  mutate(x = difftime(as.POSIXct(x, origin="1970-01-01"), start_date, units = "mins"))
}

loess.hn <- get_loess(hacker_comments_roll)
loess.rd <- get_loess(reddit_comments_roll)
loess.tw <- get_loess(twitter_comments_roll)

g <- ggplot() +
  geom_line(data = loess.hn, aes(x = x, y = y), size=1, col="darkorange") +
  geom_line(data = loess.rd, aes(x = x, y = y), size=1, col="red") +
  geom_line(data = loess.tw, aes(x = x, y = y), size=1, col="skyblue") +
  ylab("activity") + 
  xlab("minutes passed") +
  theme_minimal()

ggplotly(g)
```

Overall discussion 

```{r, warning=FALSE, echo=FALSE, message=FALSE, fig.width=10, fig.align="center"}
library(lubridate)

get_cumulative_activity_in_time <- function(df, name){
  df <- df %>% arrange(time)
  df$cumulative_activity <- as.numeric(row.names(df))
  df <- df %>% mutate(time = ymd_hms(time))
  df <- cbind(df, data.frame(site = name))
  df %>% select(site, cumulative_activity, time)
}

cumulative_activity_in_time <- rbind(
  get_cumulative_activity_in_time(twitter_data, "Twitter"),
  get_cumulative_activity_in_time(reddit_data, "Reddit"),
  get_cumulative_activity_in_time(hacker_comments, "Hacker News")
) 


g <- cumulative_activity_in_time %>% 
  ggplot(aes(x = time, y = cumulative_activity, color = site)) +
  geom_line() +
  theme_minimal() +
  geom_point(size=1) +
  scale_color_manual(values = c("skyblue", "red", "darkorange"),
                     labels = c("Twitter", "Reddit", "HackerNews")) +
  xlab("") +
  ylab("Cumulative activity") +
  theme(panel.grid.major.x  = element_blank(), 
        panel.grid.minor = element_blank())
  
  

ggplotly(g) %>% config(displayModeBar = F)

```

# Wordclouds

```{r ,warning=FALSE, echo=FALSE, message=FALSE}
library(ggwordcloud)

get_word_cloud <- function(data){
  data <- table(unlist(clear_all_sentences(data)))
  data <- data.frame(data) %>% top_n(100, Freq)
  data %>% 
    mutate(word=Var1) %>% 
    mutate(n=Freq) %>% 
    mutate(color=factor(sample(10, nrow(data), replace=TRUE))) %>% 
    ggplot(aes(label=word, size=n, color=color)) + 
      geom_text_wordcloud() +
      theme_minimal()
}
```

Hackernews

```{r echo=FALSE, warning=FALSE, message=FALSE}
get_word_cloud(hacker_comments$text)
```

Reddit

```{r message=FALSE, warning=FALSE, echo=FALSE}
get_word_cloud(reddit_data$comment)
```

Twitter

```{r  message=FALSE, warning=FALSE, echo=FALSE}
get_word_cloud(twitter_data$text)
```
